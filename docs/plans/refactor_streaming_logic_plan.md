# Implementation Plan: Refactor Agent Streaming Logic

**Version:** 1.0
**Date:** 2025-05-14
**Author(s):** Ryan, Gemini
**Related Design Document (Optional):** N/A (Derived from review of Task B.2)

## 1. Goals
*   Simplify the Server-Sent Event (SSE) indexing logic within `src/agents/agent_turn_processor.py` for the `stream_turn_response()` method.
*   Reduce backend complexity by relying more directly on the LLM provider's (e.g., Anthropic's) content block indexing.
*   Ensure the frontend (`frontend/src/features/execute/views/AgentChatView.tsx`) can still correctly consume and render the simplified stream.
*   Maintain functionality for text streaming, tool calls, and tool results.
*   Improve maintainability and clarity of the streaming code.

## 2. Scope
*   **In Scope (Backend):**
    *   `src/agents/agent_turn_processor.py`: Primarily the `stream_turn_response()` method.
    *   Potentially minor adjustments in `src/llm/providers/anthropic_client.py` if the yielded event structure needs slight changes (e.g., ensuring the LLM's original block index is consistently available).
*   **In Scope (Frontend):**
    *   `frontend/src/features/execute/views/AgentChatView.tsx`: Adjust SSE event handlers to work with the simplified indexing from the backend.
    *   `frontend/src/components/common/StreamingMessageContentView.tsx`: Ensure it can still render blocks correctly with potentially simplified or more direct indexing.
*   **Out of Scope:**
    *   Major changes to the overall agent execution flow outside of streaming.
    *   Fundamental changes to how the frontend renders individual block types (text, tool call, tool result), unless necessitated by the indexing change.
    *   Implementing new features for thinking/response separation beyond what the simplified stream naturally supports (this can be a future enhancement).

## 3. Prerequisites
*   Understanding of the current streaming logic in `AgentTurnProcessor` and `AgentChatView`.
*   Agreement on the simplified indexing approach (primarily passing through the LLM's block index).

## 4. Implementation Steps

**Phase 1: Backend Refactoring (`AgentTurnProcessor`)**

1.  **Step 1.1: Remove Complex Index Mapping Logic**
    *   **File(s):** `src/agents/agent_turn_processor.py` (method: `stream_turn_response`)
    *   **Action:**
        *   Delete the following instance variables or local variables related to custom frontend SSE index management:
            *   `frontend_sse_index_allocator`
            *   `llm_block_original_idx_to_frontend_idx`
            *   `tool_id_to_frontend_idx`
            *   `active_frontend_idx_for_current_text_stream`
        *   Remove the complex conditional logic that uses these variables to calculate `final_frontend_idx_to_yield`.
    *   **Verification:** Code inspection. Ensure no remnants of this logic interfere with the new approach.

2.  **Step 1.2: Pass Through LLM's Original Index**
    *   **File(s):** `src/agents/agent_turn_processor.py` (method: `stream_turn_response`)
    *   **Action:**
        *   When iterating through `llm_event` from `self.llm.stream_message()`:
            *   Extract the `index` from `event_data` (which is the LLM's original content block index, e.g., from Anthropic).
            *   For events that correspond to LLM content blocks (e.g., `text_block_start`, `text_delta`, `tool_use_start`, `tool_use_input_delta`, `content_block_stop`), ensure this original LLM index is the one yielded to the frontend in the `data.index` field of the SSE event.
        *   No modification of this index should occur within `AgentTurnProcessor` for these event types.
    *   **Verification:** Code inspection. Trace how the `index` from `llm_event.data` flows to the `yield` statement.

3.  **Step 1.3: Simplify Indexing for Processor-Generated Events**
    *   **File(s):** `src/agents/agent_turn_processor.py` (method: `stream_turn_response`)
    *   **Action:**
        *   For events generated by `AgentTurnProcessor` itself (e.g., `tool_result`, `tool_execution_error`, and the custom `tool_use_input_complete` if retained):
            *   **Primary Approach:** These events should primarily be identified by `tool_use_id`.
            *   **`index` Field:**
                *   If the frontend can associate these events using `tool_use_id` alone (as it seems to do for `tool_result`), the `index` field for these events might be omitted or set to the `index` of the original `tool_use` block that triggered them.
                *   If `tool_use_input_complete` is kept and needs an index for the frontend to update a specific block, it should use the original LLM index of the `tool_use` block it corresponds to.
    *   **Verification:** Code inspection. Decide on the `index` strategy for these events and implement consistently.

4.  **Step 1.4: Review and Adjust `tool_use_input_complete` Event**
    *   **File(s):** `src/agents/agent_turn_processor.py` (method: `stream_turn_response`)
    *   **Action:**
        *   Determine if the custom `tool_use_input_complete` event is still necessary. The `content_block_stop` event for a `tool_use` block (from the LLM stream) signals that the LLM is done defining the tool call. Input parsing and tool execution can occur at that point.
        *   If `tool_use_input_complete` is removed, ensure its functionality (like final input parsing) is merged into the handling of `content_block_stop` for `tool_use` blocks.
    *   **Verification:** Code inspection. Test tool call functionality after changes.

5.  **Step 1.5: Update Tool Execution Logic in Streaming**
    *   **File(s):** `src/agents/agent_turn_processor.py` (method: `stream_turn_response`)
    *   **Action:**
        *   Ensure that when a `tool_use` block is finalized (likely on `content_block_stop`), the `active_tool_calls` dictionary (keyed by the LLM's original block index) is used to retrieve the full input.
        *   Tool execution and yielding of `tool_result`/`tool_execution_error` should proceed as currently, but ensure they correctly reference the `tool_use_id`.
    *   **Verification:** Code inspection and testing of tool usage in streaming mode.

6.  **Step 1.6: Ensure `llm_call_completed` Event is Correct**
    *   **File(s):** `src/agents/agent_turn_processor.py` (method: `stream_turn_response`)
    *   **Action:** Verify that the custom `llm_call_completed` event is still yielded correctly at the end of processing events from a single `self.llm.stream_message()` call. This event is crucial for the `Agent` class's main streaming loop.
    *   **Verification:** Code inspection.

**Phase 2: Frontend Adjustments (`AgentChatView.tsx`, `StreamingMessageContentView.tsx`)**

1.  **Step 2.1: Adapt Event Handlers to New Indexing**
    *   **File(s):** `frontend/src/features/execute/views/AgentChatView.tsx`
    *   **Action:**
        *   Review handlers for `text_delta`, `tool_use_start`, `tool_use_input_delta`, `content_block_stop`.
        *   These handlers use `eventData.index` as an array index. They should now expect this `index` to be the LLM's original content block index.
        *   The logic for padding with `'placeholder'` blocks might still be necessary if the LLM can send sparse indices (e.g., send block 0 then block 2). However, Anthropic typically sends contiguous block indices starting from 0 for a given message.
        *   Verify that the `content_block_stop` handler's logic for transforming a text block into multiple sub-blocks (thinking, JSON data) still functions correctly with the direct LLM index.
    *   **Verification:** Test streaming UI with various agent responses (text, tool calls, thinking).

2.  **Step 2.2: Verify `tool_result` and `tool_execution_error` Handling**
    *   **File(s):** `frontend/src/features/execute/views/AgentChatView.tsx`
    *   **Action:**
        *   Confirm that these handlers correctly use `tool_use_id` to find the corresponding `tool_use` block and insert the result/error message after it.
        *   If the backend changes how/if an `index` is provided for these events, ensure the frontend doesn't break due to assumptions about this `index`. (Current analysis suggests it primarily uses `tool_use_id` for placement).
    *   **Verification:** Test tool calls and error handling in the UI.

3.  **Step 2.3: Review `StreamingMessageContentView.tsx`**
    *   **File(s):** `frontend/src/components/common/StreamingMessageContentView.tsx`
    *   **Action:**
        *   Ensure the component correctly renders blocks based on their `type` and the (now direct LLM) `index` used in keys.
        *   The logic for displaying different block types (`thinking_finalized`, `final_response_data`, `text`, `tool_use`, `tool_result`) should largely remain the same.
    *   **Verification:** Visual inspection of the chat UI during streaming.

**Phase 3: Testing and Documentation**

1.  **Step 3.1: Comprehensive Testing**
    *   **Action:**
        *   Manually test various scenarios:
            *   Simple text responses.
            *   Responses with `<thinking>` tags.
            *   Responses with JSON data (requiring schema validation if configured, though schema validation is part of non-streaming).
            *   Single and multiple tool calls in a turn.
            *   Tool execution errors.
            *   Max iterations reached.
        *   Focus on the streamed output in the frontend UI, ensuring correct rendering and block association.
    *   **Verification:** UI behaves as expected, no console errors related to indexing or block updates.

2.  **Step 3.2: Update Code Comments**
    *   **File(s):** `src/agents/agent_turn_processor.py`, `frontend/src/features/execute/views/AgentChatView.tsx`
    *   **Action:** Add/update comments to explain the simplified indexing logic and the flow of events.
    *   **Verification:** Code review for clarity.

3.  **Step 3.3: Update Relevant Design/Layer Documents (If Necessary)**
    *   **File(s):** `docs/layers/2_orchestration.md` or potentially a new document if the streaming logic is detailed.
    *   **Action:** Briefly document the simplified streaming approach if the changes are significant enough to warrant it.
    *   **Verification:** Documentation accurately reflects the new implementation.

## 5. Potential Risks & Mitigation
*   **Risk:** Frontend rendering issues if the direct LLM indexing behaves unexpectedly (e.g., non-contiguous indices for a single conceptual block that the old logic tried to unify).
    *   **Mitigation:** Thorough testing with diverse LLM outputs. The frontend's placeholder logic might still handle some cases.
*   **Risk:** Tool result association might break if the `tool_use_id` is not consistently handled.
    *   **Mitigation:** Ensure `tool_use_id` is correctly passed from `tool_use_start` through to `tool_result` events.

This plan provides a roadmap for simplifying the streaming logic. We can proceed with these steps once you approve.
