"""
Integration tests for the QA Engine.

This module tests the QAEngine functionality, including test case evaluation,
expectation analysis, schema validation, and backward compatibility.
"""

from datetime import datetime
from unittest.mock import MagicMock, patch

import pytest

from aurite.lib.models.api.requests import EvaluationCase, EvaluationRequest
from aurite.testing.qa.qa_engine import QAEngine, evaluate
from aurite.testing.qa.qa_models import (
    CaseEvaluationResult,
)

# Import fixtures
from tests.fixtures.qa_fixtures import (
    assert_case_result_valid,
    assert_evaluation_result_valid,
)


@pytest.mark.anyio
@pytest.mark.testing
class TestQAEngine:
    """Test suite for the QAEngine class."""

    async def test_basic_evaluation_with_provided_outputs(self, basic_evaluation_request, mock_llm_client):
        """Test basic evaluation with pre-provided outputs."""
        # Create QAEngine instance
        qa_engine = QAEngine()

        # Mock the LLM client
        with patch.object(qa_engine, "_get_llm_client", return_value=mock_llm_client):
            # Run evaluation
            result = await qa_engine.evaluate_component(basic_evaluation_request)

        # Verify result structure
        assert_evaluation_result_valid(result)

        # Check specific values
        assert result.component_type == "agent"
        assert result.component_name == "test_agent"
        assert result.total_cases == 6  # 4 weather + 2 math cases
        assert result.status in ["success", "partial", "failed"]

        # Verify all cases were evaluated
        assert len(result.case_results) == 6

        # Check that each case has a valid result
        for _case_id, case_result in result.case_results.items():
            assert_case_result_valid(case_result)

    async def test_evaluation_with_run_agent_function(self, evaluation_request_with_run_agent, mock_llm_client):
        """Test evaluation with run_agent function to generate outputs."""
        qa_engine = QAEngine()

        with patch.object(qa_engine, "_get_llm_client", return_value=mock_llm_client):
            result = await qa_engine.evaluate_component(evaluation_request_with_run_agent)

        assert_evaluation_result_valid(result)
        assert result.total_cases == 3

        # Verify outputs were generated by run_agent
        for case_result in result.case_results.values():
            assert case_result.output is not None
            # Check that the output matches what mock_run_agent would produce
            if "weather" in case_result.input.lower():
                assert "15Â°C" in case_result.output
            elif "2 + 2" in case_result.input.lower():
                assert "4" in case_result.output

    async def test_evaluation_with_async_run_agent(self, evaluation_request_with_async_run_agent, mock_llm_client):
        """Test evaluation with async run_agent function."""
        qa_engine = QAEngine()

        with patch.object(qa_engine, "_get_llm_client", return_value=mock_llm_client):
            result = await qa_engine.evaluate_component(evaluation_request_with_async_run_agent)

        assert_evaluation_result_valid(result)
        assert result.total_cases == 3

        # Verify async function was handled correctly
        for case_result in result.case_results.values():
            assert case_result.output is not None

    async def test_evaluation_with_aurite_engine(self, mock_aurite_engine, mock_llm_client):
        """Test evaluation using AuriteEngine to run components."""
        qa_engine = QAEngine()

        # Create request without run_agent (will use executor)
        request = EvaluationRequest(
            eval_name="test_agent",
            eval_type="agent",
            test_cases=[
                EvaluationCase(
                    input="What's the weather?",
                    expectations=["The output contains weather information"],
                )
            ],
        )

        with patch.object(qa_engine, "_get_llm_client", return_value=mock_llm_client):
            result = await qa_engine.evaluate_component(request, mock_aurite_engine)

        assert_evaluation_result_valid(result)
        assert result.total_cases == 1

        # Verify AuriteEngine was called
        mock_aurite_engine.run_agent.assert_called_once()

    async def test_schema_validation(self, evaluation_request_with_schema, mock_llm_client):
        """Test schema validation functionality."""
        qa_engine = QAEngine()

        with patch.object(qa_engine, "_get_llm_client", return_value=mock_llm_client):
            result = await qa_engine.evaluate_component(evaluation_request_with_schema)

        assert_evaluation_result_valid(result)

        # Check schema validation results
        case_ids = list(result.case_results.keys())

        # First case should pass schema validation
        first_result = result.case_results[case_ids[0]]
        assert first_result.schema_valid is True
        assert first_result.grade == "PASS"

        # Second case should fail (age is string instead of number)
        second_result = result.case_results[case_ids[1]]
        assert second_result.schema_valid is False
        assert second_result.grade == "FAIL"
        assert len(second_result.schema_errors) > 0

        # Third case should fail (not valid JSON)
        third_result = result.case_results[case_ids[2]]
        assert third_result.schema_valid is False
        assert third_result.grade == "FAIL"

    async def test_expectation_analysis_with_failures(self, basic_evaluation_request, mock_llm_client_with_failures):
        """Test expectation analysis with some failing cases."""
        qa_engine = QAEngine()

        with patch.object(qa_engine, "_get_llm_client", return_value=mock_llm_client_with_failures):
            result = await qa_engine.evaluate_component(basic_evaluation_request)

        assert_evaluation_result_valid(result)

        # Should have some failures based on mock_llm_client_with_failures logic
        assert result.failed_cases > 0
        assert result.status in ["partial", "failed"]

        # Check that failed cases have broken expectations
        for case_result in result.case_results.values():
            if case_result.grade == "FAIL":
                assert len(case_result.expectations_broken) > 0

    async def test_error_handling_in_evaluation(self, mock_llm_client):
        """Test error handling when evaluation fails."""
        qa_engine = QAEngine()

        # Create a request that will cause an error
        request = EvaluationRequest(
            eval_name="test_agent",
            eval_type=None,  # No eval_type and no run_agent will cause an error
            test_cases=[
                EvaluationCase(
                    input="Test",
                    expectations=["Should handle error"],
                )
            ],
        )

        with patch.object(qa_engine, "_get_llm_client", return_value=mock_llm_client):
            result = await qa_engine.evaluate_component(request, None)

        # Should still return a result, but with failed status
        assert_evaluation_result_valid(result)
        assert result.status == "failed"
        assert result.overall_score == 0

    async def test_recommendations_generation(self):
        """Test that recommendations are generated based on results."""
        qa_engine = QAEngine()

        # Create mixed results
        case_results = {
            "case_1": CaseEvaluationResult(
                case_id="case_1",
                input="Test 1",
                output="Output 1",
                grade="PASS",
                analysis="Good",
                expectations_broken=[],
            ),
            "case_2": CaseEvaluationResult(
                case_id="case_2",
                input="Test 2",
                output="Output 2",
                grade="FAIL",
                analysis="Bad",
                expectations_broken=["Expectation 1"],
            ),
            "case_3": CaseEvaluationResult(
                case_id="case_3",
                input="Test 3",
                output="Output 3",
                grade="FAIL",
                analysis="Schema failed",
                expectations_broken=[],
                schema_valid=False,
                schema_errors=["Invalid format"],
            ),
        }

        recommendations = qa_engine._generate_recommendations(list(case_results.values()))

        assert len(recommendations) > 0
        # Should mention high failure rate
        assert any("failure rate" in r.lower() for r in recommendations)
        # Should mention schema validation
        assert any("schema" in r.lower() for r in recommendations)

    async def test_backward_compatibility(self, basic_evaluation_request, mock_llm_client):
        """Test backward compatibility with the old evaluate function."""
        with patch(
            "aurite.testing.qa.qa_engine.QAEngine._get_llm_client",
            return_value=mock_llm_client,
        ):
            # Use the old evaluate function
            result = await evaluate(basic_evaluation_request)

        # Should return legacy format
        assert isinstance(result, dict)
        assert "status" in result
        assert "result" in result
        assert result["status"] in ["success", "partial", "failed"]

        # Check that result contains case evaluations
        assert isinstance(result["result"], dict)
        for _case_id, case_result in result["result"].items():
            assert "input" in case_result
            assert "output" in case_result
            assert "grade" in case_result
            assert "analysis" in case_result
            assert "expectations_broken" in case_result

    async def test_parallel_execution(self, mock_llm_client):
        """Test that test cases are executed in parallel."""
        qa_engine = QAEngine()

        # Create multiple test cases
        test_cases = [
            EvaluationCase(
                input=f"Test {i}",
                output=f"Output {i}",
                expectations=[f"Expectation {i}"],
            )
            for i in range(5)
        ]

        request = EvaluationRequest(
            eval_name="parallel_test",
            eval_type="agent",
            test_cases=test_cases,
        )

        # Track call times to verify parallel execution
        call_times = []

        async def mock_create_message(*args, **kwargs):
            call_times.append(datetime.utcnow())
            return MagicMock(content='{"analysis": "Test", "expectations_broken": []}')

        mock_llm_client.create_message = mock_create_message

        with patch.object(qa_engine, "_get_llm_client", return_value=mock_llm_client):
            result = await qa_engine.evaluate_component(request)

        assert_evaluation_result_valid(result)
        assert result.total_cases == 5

        # If executed in parallel, all calls should happen close together
        if len(call_times) > 1:
            time_diff = (call_times[-1] - call_times[0]).total_seconds()
            # Should complete within a second if parallel (would take longer if sequential)
            assert time_diff < 1.0

    async def test_execution_time_tracking(self, basic_evaluation_request, mock_llm_client):
        """Test that execution time is tracked for each case and overall."""
        qa_engine = QAEngine()

        with patch.object(qa_engine, "_get_llm_client", return_value=mock_llm_client):
            result = await qa_engine.evaluate_component(basic_evaluation_request)

        assert_evaluation_result_valid(result)

        # Check overall timing
        assert result.duration_seconds is not None and result.duration_seconds > 0
        assert result.started_at is not None
        assert result.completed_at is not None
        assert result.started_at < result.completed_at

        # Check individual case timing
        for case_result in result.case_results.values():
            if case_result.execution_time is not None:
                assert case_result.execution_time >= 0

    async def test_empty_test_cases(self, mock_llm_client):
        """Test handling of empty test cases."""
        qa_engine = QAEngine()

        request = EvaluationRequest(
            eval_name="empty_test",
            eval_type="agent",
            test_cases=[],
        )

        with patch.object(qa_engine, "_get_llm_client", return_value=mock_llm_client):
            result = await qa_engine.evaluate_component(request)

        assert_evaluation_result_valid(result)
        assert result.total_cases == 0
        assert result.overall_score == 0
        assert result.status == "success"  # No failures if no tests


@pytest.mark.anyio
@pytest.mark.testing
class TestQAModels:
    """Test suite for QA model classes."""

    def test_qa_evaluation_result_to_legacy_format(self, sample_qa_evaluation_result):
        """Test conversion to legacy format."""
        legacy = sample_qa_evaluation_result.to_legacy_format()

        assert isinstance(legacy, dict)
        assert legacy["status"] == "partial"
        assert "result" in legacy

        # Check case results are in legacy format
        for _case_id, case_data in legacy["result"].items():
            assert "input" in case_data
            assert "output" in case_data
            assert "grade" in case_data
            assert "analysis" in case_data
            assert "expectations_broken" in case_data

    def test_case_evaluation_result_validation(self):
        """Test CaseEvaluationResult model validation."""
        # Valid case result
        result = CaseEvaluationResult(
            case_id="test_1",
            input="Test input",
            output="Test output",
            grade="PASS",
            analysis="All good",
            expectations_broken=[],
        )

        assert result.case_id == "test_1"
        assert result.grade == "PASS"
        assert result.schema_valid is True  # Default value

        # Test with schema failure
        result_with_schema_error = CaseEvaluationResult(
            case_id="test_2",
            input="Test input",
            output="Invalid output",
            grade="FAIL",
            analysis="Schema validation failed",
            expectations_broken=[],
            schema_valid=False,
            schema_errors=["Not valid JSON"],
        )

        assert result_with_schema_error.schema_valid is False
        assert len(result_with_schema_error.schema_errors) == 1
